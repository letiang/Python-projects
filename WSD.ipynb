{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YwXy1ipF3cL7jsjjlp9L8F2bs0jonco5","authorship_tag":"ABX9TyPehq6ENFGsA7cZagndDE3L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5r96zvCwKZEJ","executionInfo":{"status":"ok","timestamp":1731783455468,"user_tz":300,"elapsed":781535,"user":{"displayName":"Letian Gao","userId":"10046384146796585092"}},"outputId":"72df6a3c-af9d-4ca9-a427-4c2d6ed0a107"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}],"source":["import xml.etree.cElementTree as ET\n","import codecs\n","\n","import nltk\n","from nltk.corpus import wordnet as wn\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.wsd import lesk\n","\n","import numpy as np\n","from gensim.models import KeyedVectors\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import gensim.downloader as api\n","\n","nltk.download(\"wordnet\")\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt_tab\")\n","word2vec = api.load(\"word2vec-google-news-300\")\n"]},{"cell_type":"code","source":["class WSDInstance:\n","    def __init__(self, my_id, lemma, context, index):\n","        self.id = my_id         # id of the WSD instance\n","        self.lemma = lemma      # lemma of the word whose sense is to be resolved\n","        self.context = context  # lemma of all the words in the sentential context\n","        self.index = index      # index of lemma within the context\n","    def __str__(self):\n","        '''\n","        For printing purposes.\n","        '''\n","        return '%s\\t%s\\t%s\\t%d' % (self.id, self.lemma, ' '.join(self.context), self.index)\n","\n","def load_instances(f):\n","    '''\n","    Load two lists of cases to perform WSD on. The structure that is returned is a dict, where\n","    the keys are the ids, and the values are instances of WSDInstance.\n","    '''\n","    tree = ET.parse(f)\n","    root = tree.getroot()\n","\n","    dev_instances = {}\n","    test_instances = {}\n","\n","    for text in root:\n","        if text.attrib['id'].startswith('d001'):\n","            instances = dev_instances\n","        else:\n","            instances = test_instances\n","        for sentence in text:\n","            # construct sentence context\n","            context = [to_ascii(el.attrib['lemma']) for el in sentence]\n","            context = [word.decode('utf-8') if isinstance(word, bytes) else word for word in context]\n","            for i, el in enumerate(sentence):\n","                if el.tag == 'instance':\n","                    my_id = el.attrib['id']\n","                    my_id = my_id.decode('utf-8') if isinstance(my_id,bytes) else my_id\n","                    lemma = to_ascii(el.attrib['lemma'])\n","                    lemma = lemma.decode('utf-8') if isinstance(lemma,bytes) else lemma\n","                    lemma = lemma.lower()\n","                    instances[my_id] = WSDInstance(my_id, lemma, context, i)\n","    return dev_instances, test_instances\n","\n","def load_key(f):\n","    '''\n","    Load the solutions as dicts.\n","    Key is the id\n","    Value is the list of correct sense keys.\n","    '''\n","    dev_key = {}\n","    test_key = {}\n","    for line in open(f):\n","        if len(line) <= 1: continue\n","        #print (line)\n","        doc, my_id, sense_key = line.strip().split(' ', 2)\n","        if doc == 'd001':\n","            dev_key[my_id] = sense_key.split()\n","        else:\n","            test_key[my_id] = sense_key.split()\n","    return dev_key, test_key\n","\n","def to_ascii(s):\n","    # remove all non-ascii characters\n","    return codecs.encode(s, 'ascii', 'ignore')\n","\n","\n","\n","\n","# Preprocess a sentence\n","def preprocess_sentence(sentence):\n","    \"\"\"\n","    Preprocess a sentence: tokenize, lemmatize, remove stopwords, and handle multi-word phrases.\n","    \"\"\"\n","    lemmatizer = WordNetLemmatizer()\n","    stop_words = set(stopwords.words(\"english\"))\n","\n","    tokens = word_tokenize(sentence.lower())  # Lowercase for consistency\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]\n","    tokens = [token.replace(\"_\", \" \") for token in tokens]  # Convert underscores to spaces\n","\n","    return [token for token in tokens if token not in stop_words]\n","\n","\n","# Most Frequent Sense\n","def most_frequent_sense(lemma):\n","    \"\"\"\n","    Retrieve the most frequent sense of a word from WordNet.\n","    \"\"\"\n","    synsets = wn.synsets(lemma)\n","    return synsets[0] if synsets else None\n","\n","# Lesk's Algorithm\n","def lesk_method(context, lemma):\n","    \"\"\"\n","    Apply Lesk's algorithm using NLTK.\n","    \"\"\"\n","    return lesk(context, lemma)\n","\n","def synset_to_sense_keys(synset):\n","    \"\"\"\n","    Convert a WordNet synset to its corresponding lemma sense keys.\n","    \"\"\"\n","    return [lemma.key() for lemma in synset.lemmas()]\n","\n","\n","# Evaluate WSD methods\n","def evaluate_wsd_with_keys(instances, keys):\n","    \"\"\"\n","    Evaluate Most Frequent Sense and Lesk methods on the dataset,\n","    using lemma sense keys for comparison.\n","    \"\"\"\n","    correct_mfs = 0\n","    correct_lesk = 0\n","    total_cases = len(instances)\n","\n","    for instance_id, instance in instances.items():\n","        lemma = instance.lemma\n","        context = instance.context\n","        gold_sense_keys = keys.get(instance_id, [])\n","\n","        # Most Frequent Sense\n","        mfs_sense = most_frequent_sense(lemma)\n","        if mfs_sense:\n","            mfs_sense_keys = synset_to_sense_keys(mfs_sense)\n","            if any(key in gold_sense_keys for key in mfs_sense_keys):\n","                correct_mfs += 1\n","\n","        # Lesk Algorithm\n","        lesk_sense = lesk_method(context, lemma)\n","        if lesk_sense:\n","            lesk_sense_keys = synset_to_sense_keys(lesk_sense)\n","            if any(key in gold_sense_keys for key in lesk_sense_keys):\n","                correct_lesk += 1\n","\n","    mfs_accuracy = correct_mfs / total_cases\n","    lesk_accuracy = correct_lesk / total_cases\n","\n","    return mfs_accuracy, lesk_accuracy\n","\n","\n","\n","def lesk_disambiguate_tune(instance, pos_tag):\n","    # Tokenize, lemmatize, and remove stopwords from the context\n","    context = preprocess_sentence(' '.join(instance.context))\n","\n","    # Apply Lesk's algorithm for the specified POS tag\n","    sense = lesk(context, instance.lemma, pos_tag)\n","\n","    # Extract only the lemma from the predicted sense\n","    predicted_sense = sense.name().split('.')[0] if sense else None\n","\n","    return predicted_sense\n","\n","best_accuracy = 0\n","pos_tags_to_evaluate = ['n', 'v', 'a']  # You can add more POS tags as needed\n","\n","for pos_tag in pos_tags_to_evaluate:\n","        # Use Lesk's algorithm on the dev set for the current POS tag\n","        dev_predictions_lesk = {k: lesk_disambiguate_tune(v, pos_tag) for k, v in dev_instances.items()}\n","\n","        # Evaluate accuracy for Lesk's algorithm for the current POS tag\n","        accuracy_lesk = sum(1 for k, v in dev_predictions_lesk.items() if v in [sense.split('%')[0] for sense in dev_key.get(k, [])]) / len(dev_predictions_lesk)\n","\n","        accuracy_percentage = accuracy_lesk * 100\n","        print(f'Accuracy for Lesk\\'s algorithm ({pos_tag}): {accuracy_percentage:.2f}%')\n","\n","\n","        # Update the best accuracy and POS tag if the current accuracy is higher\n","        if accuracy_lesk > best_accuracy:\n","            best_accuracy = accuracy_lesk\n","            best_pos_tag = pos_tag\n","\n","    # Print the result with the highest accuracy\n","print(f'Highest Accuracy: {best_accuracy * 100:.2f}% (POS Tag: {best_pos_tag})')\n","\n","\n","\n","\n","def get_embedding(word, model):\n","    \"\"\"\n","    Get the Word2Vec embedding for a word. Return a zero vector if the word is not in the vocabulary.\n","    \"\"\"\n","    if word in model:\n","        return model[word]\n","    return np.zeros(model.vector_size)\n","\n","\n","def compute_sense_embedding(synset, model):\n","    \"\"\"\n","    Compute the embedding for a WordNet synset based on its definition and examples.\n","    \"\"\"\n","    definition = synset.definition()\n","    examples = synset.examples()\n","    words = preprocess_sentence(definition + \" \" + \" \".join(examples))\n","    embeddings = [get_embedding(word, model) for word in words if word in model]\n","    if embeddings:\n","        return np.mean(embeddings, axis=0)\n","    return np.zeros(model.vector_size)\n","\n","\n","def compute_context_embedding(context, target_index, model, window=3):\n","    \"\"\"\n","    Compute the embedding for the context around a target word.\n","    \"\"\"\n","    start = max(0, target_index - window)\n","    end = min(len(context), target_index + window + 1)\n","    words = [word for i, word in enumerate(context) if i != target_index]\n","    embeddings = [get_embedding(word, model) for word in words if word in model]\n","    if embeddings:\n","        return np.mean(embeddings, axis=0)\n","    return np.zeros(model.vector_size)\n","\n","\n","def word2vec_wsd(instance, model):\n","    \"\"\"\n","    Perform WSD using pre-trained Word2Vec embeddings.\n","    \"\"\"\n","    lemma = instance.lemma\n","    context = preprocess_sentence(\" \".join(instance.context))\n","    target_index = instance.index\n","\n","    context_embedding = compute_context_embedding(context, target_index, model)\n","    if np.linalg.norm(context_embedding) == 0:\n","        return None  # No context embedding available\n","\n","    best_sense = None\n","    max_similarity = -1\n","\n","    for synset in wn.synsets(lemma):\n","        sense_embedding = compute_sense_embedding(synset, model)\n","        if np.linalg.norm(sense_embedding) == 0:\n","            continue\n","        similarity = cosine_similarity(\n","            [context_embedding], [sense_embedding]\n","        )[0][0]\n","        if similarity > max_similarity:\n","            max_similarity = similarity\n","            best_sense = synset\n","\n","    return best_sense\n","\n","def evaluate_wsd_with_word2vec(instances, keys, model):\n","    \"\"\"\n","    Evaluate the Word2Vec-based WSD method on the given instances and keys.\n","    \"\"\"\n","    correct = 0\n","    total = len(instances)\n","\n","    for instance_id, instance in instances.items():\n","        gold_sense_keys = keys.get(instance_id, [])\n","        predicted_synset = word2vec_wsd(instance, model)\n","\n","        if predicted_synset:\n","            predicted_sense_keys = [lemma.key() for lemma in predicted_synset.lemmas()]\n","            if any(key in gold_sense_keys for key in predicted_sense_keys):\n","                correct += 1\n","\n","    accuracy = correct / total\n","    return accuracy\n","\n","# Main script\n","if __name__ == \"__main__\":\n","    # File paths (update these to your dataset's actual paths)\n","    data_f = '/content/drive/MyDrive/multilingual-all-words.en.xml'\n","    key_f = '/content/drive/MyDrive/wordnet.en.key'\n","\n","    # Load instances and keys\n","    dev_instances, test_instances = load_instances(data_f)\n","    dev_key, test_key = load_key(key_f)\n","\n","    # Filter instances not in keys\n","    dev_instances = {k: v for k, v in dev_instances.items() if k in dev_key}\n","    test_instances = {k: v for k, v in test_instances.items() if k in test_key}\n","\n","    # Evaluate on dev and test sets\n","    dev_mfs_acc, dev_lesk_acc = evaluate_wsd_with_keys(dev_instances, dev_key)\n","    print(f\"Development Set Accuracy (Most Frequent Sense): {dev_mfs_acc * 100:.2f}%\")\n","    print(f\"Development Set Accuracy (Lesk): {dev_lesk_acc * 100:.2f}%\")\n","\n","    test_mfs_acc, test_lesk_acc = evaluate_wsd_with_keys(test_instances, test_key)\n","    print(f\"Test Set Accuracy (Most Frequent Sense): {test_mfs_acc * 100:.2f}%\")\n","    print(f\"Test Set Accuracy (Lesk): {test_lesk_acc * 100:.2f}%\")\n","\n","    print(\"Evaluating Development Set...\")\n","    dev_accuracy = evaluate_wsd_with_word2vec(dev_instances, dev_key, word2vec)\n","    print(f\"Development Set Accuracy: {dev_accuracy * 100:.2f}%\")\n","\n","    print(\"\\nEvaluating Test Set...\")\n","    test_accuracy = evaluate_wsd_with_word2vec(test_instances, test_key, word2vec)\n","    print(f\"Test Set Accuracy: {test_accuracy * 100:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5N4S5JVSSa2F","executionInfo":{"status":"ok","timestamp":1731800401935,"user_tz":300,"elapsed":29990,"user":{"displayName":"Letian Gao","userId":"10046384146796585092"}},"outputId":"389888f5-967f-4ccb-ce7f-a8b1235518dd"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for Lesk's algorithm (n): 49.34%\n","Accuracy for Lesk's algorithm (v): 18.94%\n","Accuracy for Lesk's algorithm (a): 2.20%\n","Highest Accuracy: 49.34% (POS Tag: n)\n","Development Set Accuracy (Most Frequent Sense): 67.53%\n","Development Set Accuracy (Lesk): 34.02%\n","Test Set Accuracy (Most Frequent Sense): 62.34%\n","Test Set Accuracy (Lesk): 34.07%\n","Evaluating Development Set...\n","Development Set Accuracy: 47.94%\n","\n","Evaluating Test Set...\n","Test Set Accuracy: 48.55%\n"]}]},{"cell_type":"code","source":["import random\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import accuracy_score\n","import ast\n","\n","def load_unlabeled_data(file_path):\n","    with open(file_path, \"r\") as f:\n","        lines = f.readlines()\n","    return [preprocess_sentence(line.strip()) for line in lines]\n","\n","def load_annotated_data(file_path):\n","    # Load annotated data from a file where each line contains a tuple of (sentence, label).\n","    annotated_data = []\n","    try:\n","        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","            for line in file:\n","                line = line.strip()\n","                if not line:\n","                    continue  # Skip empty lines\n","                try:\n","                    # Use `ast.literal_eval` to safely evaluate the tuple\n","                    sentence_label_tuple = ast.literal_eval(line)\n","                    if isinstance(sentence_label_tuple, tuple) and len(sentence_label_tuple) == 2:\n","                        annotated_data.append(sentence_label_tuple)\n","                    else:\n","                        raise ValueError(f\"Invalid format in line: {line}\")\n","                except Exception as e:\n","                    print(f\"Error parsing line: {line}. Error: {e}\")\n","    except FileNotFoundError:\n","        print(f\"Error: The file {file_path} does not exist.\")\n","\n","    return annotated_data\n","\n","training_set = [\n","    (\"The fishermen sat by the bank of the river, waiting for a catch.\", \"bank.n.01\"),  # Geography\n","    (\"The river's bank was covered in lush green grass.\", \"bank.n.01\"),\n","    (\"A flock of birds rested on the bank of the tranquil stream.\", \"bank.n.01\"),\n","    (\"The children played on the muddy bank after the rainstorm.\", \"bank.n.01\"),\n","    (\"The hikers paused on the bank to admire the flowing water.\", \"bank.n.01\"),\n","    (\"The boat drifted close to the bank before coming to a stop.\", \"bank.n.01\"),\n","    (\"She deposited her paycheck into her savings account at the bank.\", \"bank.n.02\"),  # Financial\n","    (\"The bank approved their loan application for the new house.\", \"bank.n.02\"),\n","    (\"The investment bank advised the corporation on its merger.\", \"bank.n.02\"),\n","    (\"He withdrew cash from the ATM located inside the bank.\", \"bank.n.02\"),\n","    (\"The bank offers competitive interest rates on savings accounts.\", \"bank.n.02\"),\n","    (\"The local bank recently opened a new branch in the city.\", \"bank.n.02\"),\n","    (\"The blood bank ensures that there is always a supply for emergencies.\", \"bank.n.03\"),  # Storage\n","    (\"She donated her old books to the library's book bank.\", \"bank.n.03\"),\n","    (\"The seed bank preserves rare plant species for future generations.\", \"bank.n.03\"),\n","    (\"The knowledge bank is a valuable resource for researchers.\", \"bank.n.03\"),\n","    (\"They relied on the food bank to get through the tough month.\", \"bank.n.03\"),\n","    (\"The power bank kept his phone charged during the road trip.\", \"bank.n.03\"),\n","    (\"The airplane began to bank sharply to the left.\", \"bank.v.01\"),  # To tilt or incline\n","    (\"The pilot banked the aircraft to avoid turbulence.\", \"bank.v.01\"),\n","    (\"The racing car banked as it sped around the sharp curve.\", \"bank.v.01\"),\n","    (\"The glider banked slightly, following the flow of the wind.\", \"bank.v.01\"),\n","    (\"The bird banked sharply to avoid the predator chasing it.\", \"bank.v.01\"),\n","    (\"The plane banked and descended toward the runway.\", \"bank.v.01\"),\n","]\n","\n","\n","def yarowsky_bootstrapping_wordnet(training_set, unlabeled_data, confidence_threshold=0.8):\n","    \"\"\"\n","    Implements Yarowsky's algorithm with logistic regression and bootstrapping using WordNet senses.\n","    \"\"\"\n","    # Separate seed data into features and labels\n","    seed_texts = [text for text,label in training_set]\n","    seed_labels = [label for text,label in training_set]\n","    # Initialize vectorizer\n","    vectorizer = CountVectorizer()\n","    X_seed = vectorizer.fit_transform(seed_texts)\n","    y_seed = np.array(seed_labels)\n","\n","    # Initialize Logistic Regression\n","    model = LogisticRegression(C= 1,max_iter=100)\n","\n","    # Iterative bootstrapping\n","    for iteration in range(5):\n","        print(f\"Iteration {iteration + 1}\")\n","\n","        # Train on the current labeled data\n","        model.fit(X_seed, y_seed)\n","\n","        # Predict on unlabeled data\n","        X_unlabeled = vectorizer.transform(unlabeled_data)\n","        probs = model.predict_proba(X_unlabeled)\n","        predictions = model.predict(X_unlabeled)\n","\n","        # Select confident predictions\n","        confident_indices = np.where(probs.max(axis=1) >= confidence_threshold)[0]\n","        confident_texts = [unlabeled_data[i] for i in confident_indices]\n","        confident_labels = [predictions[i] for i in confident_indices]\n","\n","\n","        if not confident_texts:\n","            print(\"No confident predictions, stopping early.\")\n","            break\n","\n","        # Update seed set with confident predictions\n","        X_new = vectorizer.transform(confident_texts)\n","        X_seed = np.vstack((X_seed, X_new))\n","        y_seed = np.hstack((y_seed, confident_labels))\n","\n","        # Remove confident predictions from unlabeled data\n","        unlabeled_data = [text for i, text in enumerate(unlabeled_data) if i not in confident_indices]\n","\n","        print(f\"Added {len(confident_texts)} new examples to the seed set.\")\n","\n","        # Terminate if no unlabeled data remains\n","        if not unlabeled_data:\n","            print(\"All unlabeled data processed, stopping.\")\n","            break\n","\n","    return model, vectorizer\n","\n","def evaluate_model_wordnet(model, vectorizer, test_instances):\n","    \"\"\"\n","    Evaluate the trained model on labeled test instances using WordNet synsets.\n","    \"\"\"\n","    test_texts = [text for text, label in test_instances]\n","    test_labels = [label for text, label in test_instances]\n","\n","    test_texts = [preprocess_sentence(text) for text in test_texts]\n","\n","    test_texts = [\" \".join(row) for row in test_texts]\n","    X_test = vectorizer.transform(test_texts)\n","\n","    # Predict and calculate accuracy\n","    y_pred = model.predict(X_test)\n","\n","    # Display overall accuracy\n","    accuracy = accuracy_score(test_labels, y_pred)\n","    print(f\"\\n--- Final Test Accuracy: {accuracy:.2%} ---\")\n","\n","test_set = [\n","    (\"He slipped and fell on the muddy bank of the lake.\", \"bank.n.01\"),\n","    (\"A flock of geese gathered on the bank of the stream.\", \"bank.n.01\"),\n","    (\"The tree's roots extended down to the bank of the creek.\", \"bank.n.01\"),\n","    (\"The erosion caused the bank of the river to collapse.\", \"bank.n.01\"),\n","    (\"The river overflowed, flooding the bank near the village.\", \"bank.n.01\"),\n","    (\"A picnic area was set up on the bank of the peaceful pond.\", \"bank.n.01\"),\n","    (\"The fishermen relaxed on the bank as they waited for a bite.\", \"bank.n.01\"),\n","    (\"The kayakers paddled close to the bank to avoid the strong current.\", \"bank.n.01\"),\n","    (\"He withdrew some cash from the bank's ATM.\", \"bank.n.02\"),\n","    (\"The bank offers competitive interest rates on savings accounts.\", \"bank.n.02\"),\n","    (\"The investment bank handled the company's IPO.\", \"bank.n.02\"),\n","    (\"They visited the bank to discuss their mortgage options.\", \"bank.n.02\"),\n","    (\"The bank recently introduced a new credit card with no annual fees.\", \"bank.n.02\"),\n","    (\"The local bank opened a new branch in the neighboring town.\", \"bank.n.02\"),\n","    (\"The customer was impressed with the bank's mobile app.\", \"bank.n.02\"),\n","    (\"The bank is known for its excellent customer service.\", \"bank.n.02\"),\n","    (\"The seed bank preserves plant species for future generations.\", \"bank.n.03\"),\n","    (\"They maintained a data bank of customer preferences.\", \"bank.n.03\"),\n","    (\"The knowledge bank provides resources for research and development.\", \"bank.n.03\"),\n","    (\"The power bank kept his phone charged during the trip.\", \"bank.n.03\"),\n","    (\"The blood bank issued an urgent call for donations.\", \"bank.n.03\"),\n","    (\"The library created a book bank to support underprivileged students.\", \"bank.n.03\"),\n","    (\"The researchers accessed a data bank to analyze historical trends.\", \"bank.n.03\"),\n","    (\"The hikers carried a power bank to keep their devices charged.\", \"bank.n.03\"),\n","    (\"The eagle banked gracefully as it glided through the air.\", \"bank.v.01\"),\n","    (\"He watched the jet bank and descend toward the runway.\", \"bank.v.01\"),\n","    (\"The drone banked sharply to avoid the obstacle.\", \"bank.v.01\"),\n","    (\"The plane banked as it prepared for landing.\", \"bank.v.01\"),\n","    (\"The pilot banked the airplane to avoid turbulence.\", \"bank.v.01\"),\n","    (\"The racing car banked as it sped around the sharp corner.\", \"bank.v.01\"),\n","    (\"The glider banked smoothly, following the wind currents.\", \"bank.v.01\"),\n","    (\"The bird banked sharply to avoid the oncoming predator.\", \"bank.v.01\"),\n","]\n","\n","unlabeled_data = load_unlabeled_data(\"/content/drive/MyDrive/bank.txt\")\n","unlabeled_data = [\" \".join(row) for row in unlabeled_data]\n","model, vectorizer = yarowsky_bootstrapping_wordnet(training_set, unlabeled_data)\n","evaluate_model_wordnet(model, vectorizer, test_set)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTOhqWg3adA9","executionInfo":{"status":"ok","timestamp":1731800421540,"user_tz":300,"elapsed":136,"user":{"displayName":"Letian Gao","userId":"10046384146796585092"}},"outputId":"a3a9ab4f-24d3-4cbb-dced-75504f268bc3"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1\n","No confident predictions, stopping early.\n","\n","--- Final Test Accuracy: 56.25% ---\n"]}]}]}